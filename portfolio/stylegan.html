<!DOCTYPE html>
<html >
  <head>
    <meta charset="UTF-8">

    <title>Naoki Yokoyama</title>

    <!--Favicon-->
    <link rel="icon" type="../image/png" href="../img/favicon.png">
    <!--CSS Links-->
    <link rel="stylesheet" href="../css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">    
    <link rel="stylesheet" href="../css/landing.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Heebo:light">
    <!--Scripts-->
    <script type="text/javascript" src="../js/analytics.js"></script>
    <script type="text/javascript" src="../js/jquery-2.1.3.min.js"></script>
  </head>

  <body>

    <div class="container">
      <div class="row">
        <div class="col-md-2"></div>
        <div class="col-md-8">

          <a class="title" href="../index.html">Naoki Yokoyama</a>
          <hr>
          <a href="../index.html#bio">About Me</a> &ensp;
          <a href="https://drive.google.com/file/d/1j0MELX5lTbUaxXdSLYF2bRDGz9wDlom2/view">CV</a> &ensp;
          <a href="../index.html#research">Research</a> &ensp;
          <a href="https://github.com/naokiyokoyama">GitHub</a> &ensp;
          <a href="https://scholar.google.com/citations?user=26MOv8wAAAAJ&hl=en">Google Scholar</a> &ensp;
          <br><br>
          <div class="panel-body">
            <div class="page-header">
              <center>
                <h2>StyleGAN</h2>
              </center>
            </div>
            <center>
              <img src="../img/highres_srgan/style_gan_results.png" alt="" width="60%">
              <br>
              <i class="fontsize">Faces generated by NVIDIA's StyleGAN framework, trained on 8 Tesla V100 GPUs for 6 days 14 hours.</i>
              <br>
            </center>

            <h3>Generating very realistic fake images</h3>
            <p class="fontsize">
              The main aspects of NVIDIA's new StyleGAN architecture (Karras, Laine, & Aila, 2018) allowing it to generate such realistic faces are:
            </p>
            <ul list-style-position='outside'>
              <li class="fontsize">NVIDIA's new dataset, <b>FFHQ, is comprised of 70,000 high quality 1024x1024 images of a diverse set of faces collected from Flickr</b> and refined with Mechanical Turk. Their previous dataset, CelebA-HQ, which was built off of The Univ. of Hong Kong's CelebA dataset, only contained <b>30,000 images of celebrities</b>, and was artificially augmented using a convolutional residual autoencoder (similar to RED-Net) and an SRGAN.</li>
              <li class="fontsize">An 8-layer multilayer perceptron <b>mapping the input vector to a different latent space</b>, from which "styles" are sampled for each convolutional layer in the synthesis network of the generator. They claim that this latent space features better disentanglement and is perceptually more linear than the input space.</li>
              <li class="fontsize"><b>Layer-wise injection of input data and noise</b>.</li>
              <li class="fontsize">Like they did in their previous breakthrough in generating realistic faces with GANs, the synthesis component of the generator and the discriminator were <b>gradually grown during training</b> to progressively output/assess images of higher resolution, which speeds up and greatly stabilizes training.</li>
            </ul>

            <h3>Embedding the input latent code</h3>
            <p class="fontsize">
              The generator of NVIDIA's StyleGAN is split into two networks, the <b>mapping network</b> and the <b>synthesis network</b>. Unlike traditional GANs, StyleGAN embeds the input code into an <b>intermediate latent space</b> which is done by the mapping network. The new resulting code is then <b>injected into each convolutional layer</b> of the synthesis network of the generator.
            </p>
            <center>
              <img src="../img/highres_srgan/traditional_vs_stylegan.png" alt="" width="60%">
            </center>
            <ul list-style-position='outside'>
              <li class="fontsize">The mapping network is an 8-layer multilayer perceptron with an output dimensionality of 512.</li>
              <li class="fontsize">The input code, which lies in latent space <b>z</b>, is mapped to latent space <b>w</b>.</li>
              <li class="fontsize">Since the training process enforces the sampling probability of the possible codes from <b>z</b> to match the corresponding density found in the training set, the paper argues that <b>some degree of entanglement between the factors of z is unavoidable.</b></li>
              <li class="fontsize">However, since the latent space <b>w</b> is not restricted in the same way, it is allowed <b>and even encouraged</b> to be disentangled. The paper claims that this is because it should be easier for the network to decode disentangled representations than entangled ones.</li>
              <li class="fontsize">Therefore, StyleGAN in theory generates a less entangled latent space <b>w</b>, even when the factors of variation are not known in advance.</li>
              <li class="fontsize">The output of the mapping network is injected <b>into the output of each of the synthesis network's convolutional layers</b>, after being processed by a learned affine transform (i.e. a single fully connected layer) <b>specific to each convolutional layer.</b></li>
              <li class="fontsize">As per NVIDIA's original paper on progressive GANs, they again reject the notion of that GANs suffer from covariate shift. Instead of using batch normalizaton, they use <b>adaptive instance normalization (AdaIn)</b> (Huang & Belongie, 2017), a faster and more efficient normalization technique. They use the aforementioned affine transformation to generate the scale and bias for the AdaIn operation.</li>
            </ul>

            <h3>Layer-wise style inputs</h3>
            <center>
              <img src="../img/highres_srgan/style_layers.png" alt="" width="60%">
              <p class="fontsize"><i>Mixing regularization: Visualization of different scales of style mixing. The latent codes for the generated images on the left are partially overwritten by the latent codes for the generated images on the top to generate the other images.</i></p>
            </center>
            <ul list-style-position='outside'>
              <li class="fontsize">Since the code from the latent space <b>w</b> is inputted into each convolutional layer of the synthesis network, scale-specific modifications to the generated images can be made. Each of these inputs are referred to as "styles" since similar network architectures have already been used for style transfer, image-to-image translation, and domain mixture. </li>
              <li class="fontsize">As each of the convolutional layers of the synthesis network is followed by a 2x upscaling layer, <b>each style affects the resulting image at a different scale</b>.</li>
              <li class="fontsize">The <b>AdaIn operation also serves to normalize</b> feature maps and overwrite preceding style inputs.</li>
              <li class="fontsize">To further localize the effects of the input styles, a portion of the input latent codes during training undergo <b>mixing regularization</b>, in which two latent <b>z</b> vectors are used to generate an image to attempt to fool the discriminator. This is done by using the two <b>z</b> vectors to generate two <b>w</b> vectors (<b>w1, w2</b>)and then generating a certain portion of the input styles using <b>w1</b> and the rest using <b>w2</b>. The crossover point where the source <b>w</b> vector for the input styles switches from <b>w1</b> to <b>w2</b> is randomly selected.</li>
              <li class="fontsize">The localized effects of each style is illustrated in the above figure the demonstrates the results from mixture regularization.</li>
            </ul>

            <h3>Layer-wise stochastic inputs</h3>
            <center>
              <img src="../img/highres_srgan/stochastic_results.png" alt="" width="60%">
              <p class="fontsize"><i>Input noise govern acceptably stochastic aspects of the generated photos, such as specific placement of hair strands, pores, and reflections in the eyes. Images to the far right represent the standard deviation of each pixel over 100 different realizations of input noise at each layer, highlighting which aspects are affected by the noise. </i></p>
            </center>
            <ul list-style-position='outside'>
              <li class="fontsize"><b>Many aspects of human faces can be acceptably stochastic</b>, such as the exact placement of hair/pores/freckles.</li>
              <li class="fontsize">Since a <b>traditional GAN only accepts input through its initial input layer</b>, it must learn to <b>generate pseudo-random numbers from earlier activations whenever needed in order to produce these features</b>. This takes up part of the network's capacity to learn, and hiding the periodicity of the generated numbers is difficult, as shown by many repetitive patterns that are exhibited by traditional GANs.</li>
              <li class="fontsize">By inputting <b>noise directly into each layer</b>, StyleGAN is relieved of this duty, and learns to <b>use the noise to only adjust aspects of the image that can be slightly randomized</b>, and does not exhibit the periodicity that a traditional GAN would.</li>
              <li class="fontsize">Since a <b>fresh set of noise is provided to each layer</b>, the effects of each input noise is also tightly localized to its own layer, meaning that adjusting noise at a finer layer only affects finer details (ex. placement of skin pores) while coarser layers affect only coarse features (ex. large-scale hair curling and larger background features).</li>
            </ul>

            <h3>Evaluation of new mechanisms</h3>
            <center>
              <img src="../img/highres_srgan/fid.png" alt="" width="80%">
              <p class="fontsize"><i>FID (lower is better) between real and generated images resulting from applying various techniques introduced by the paper</i></p>
            </center>
            <ul list-style-position='outside'>
              <li class="fontsize">The Fréchet Inception Distance is a metric obtained by using an InceptionV3 image classifier trained on ImageNet. The distributions between the activations generated by the InceptionV3 network on each set of images (generated and real) are then compared using the Fréchet Distance.</li>
            </ul>

<!--             <h3>The need for a better reconstruction loss for SRGANs</h3>
            <center>
              <img src="../img/highres_srgan/mrgan_results.png" alt="" width="80%">
              <p class="fontsize"><i>Results from the MR-GAN paper</i></p>
            </center>
            <ul list-style-position='outside'>
              <li class="fontsize">MR-GAN argues that using the reconstruction loss in a conditional SRGAN is a major cause of mode collapse (Lee, Ha, & Kim, 2019).</li>
              <li class="fontsize">As the Noise2Noise paper puts, the problem with a simple reconstruction loss is that "a low-resolution image <i>x</i> can be explained by many different high-resolution images <i>y</i>... the network learns to putput the average of all plausible explanations (e.g., edges shifted by different amounts), which results in spatial blurriness for the network's predictions." (Letinen et. al, 2018)</li>
            </ul>
 -->
            <!-- <h3>Experimenting with improving SRGAN performance</h3>
            <p class="fontsize">
              Currently, I'm trying to build an SRGAN in TensorFlow based on the architecture of StyleGAN. I believe that a lot of the benefits provided by the StyleGAN architecture are applicable to the task of super-resolution. However, I would like to avoid using the conventional reconstruction loss. My ideas to implement this include:
            </p>
            <ul list-style-position='outside'>
              <li class="fontsize">Reconstructing the mapping network to take in low-resolution images as inputs to map into the latent space <b>w</b> rather than taking in latent vectors.</li>
              <li class="fontsize">Instead of penalizing the network by directly comparing the output of the generator with the high-resolution ground truth, the generated image will be downscaled (using bicubic interpolation) and compared to the low-resolution input. The generated image (before bicubic interpolation) will still be evaluated using the discriminator, which will simply try to decide whether or not the image comes from the original dataset or is a product of the generator. I'm choosing to do this because while a low-resolution image can be explained by many high-resolution images, when that high-resolution image is downscaled back into the size of the low-resolution input, it should match the original input.</li>
              <li class="fontsize">Training on the FFHQ dataset.</li>
            </ul> -->
            <h3>References</h3>
            <ul list-style-position='outside'>
              <li class="fontsize">Karras, T., Laine, S., and Aila, T. (2018). A Style-Based Generator Architecture for Generative Adversarial Networks. arXiv preprint arXiv:1812.04948.</li>
              <li class="fontsize">Lee, S., Ha, J., and Kim, G. (2019). Harmonizing Maximum LIkelihood with GANs for Multimodal Conditional Generation. arXiv preprint arXiv:1902.09225.</li>
              <li class="fontsize">Lehtinen, J., Munkberg, J., Hasselgren, J., Laine, S., Karras, T., Aittala, M., and Aila, T. (2018). Noise2Noise: Learning Image Restoration without Clean Data. arXiv preprint arXiv:1803.04189.</li>
              <li class="fontsize">Huang, X., and Belongie, S. (2017). Arbitrary style transfer in real-time with adaptive instance normalization. arXiv preprint arXiv:1703.06868.</li>
            </ul>
              
          </div>
          <div class="col-md-2"></div>
      </div>
    </div>  
    <script src="../js/bootstrap.js"></script>
  </body>
</html>