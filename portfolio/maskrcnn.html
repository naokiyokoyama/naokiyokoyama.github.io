<!DOCTYPE html>
<html >
  <head>
    <meta charset="UTF-8">

    <title>Naoki Yokoyama</title>

    <!--Favicon-->
    <link rel="icon" type="../image/png" href="../img/favicon.png">
    <!--CSS Links-->
    <link rel="stylesheet" href="../css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">    
    <link rel="stylesheet" href="../css/landing.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Heebo:light">
    <!--Scripts-->
    <script type="text/javascript" src="../js/analytics.js"></script>
    <script type="text/javascript" src="../js/jquery-2.1.3.min.js"></script>
  </head>

  <body>

    <div class="container">
      <div class="row">
        <div class="col-md-2"></div>
        <div class="col-md-8">

          <a class="title" href="../index.html">Naoki Yokoyama</a>
          <hr>
          <a href="../index.html#bio">About Me</a> &ensp;
          <a href="https://drive.google.com/file/d/1j0MELX5lTbUaxXdSLYF2bRDGz9wDlom2/view">CV</a> &ensp;
          <a href="../index.html#research">Research</a> &ensp;
          <a href="https://github.com/naokiyokoyama">GitHub</a> &ensp;
          <a href="https://scholar.google.com/citations?user=26MOv8wAAAAJ&hl=en">Google Scholar</a> &ensp;
          <br><br>
          <div class="page-header">
            <center>
              <h2>Rapidly Creating an Effective, Labelled, and Diverse Training Set for Object Segmentation using Deep Learning</h2>
            </center>
          </div>
          <center>
            <img src="../img/maskrcnn/wrs_maskrcnn.png" alt="" width="80%">
            <br>
            <i class="fontsize">After training, our model was robust in detecting objects regardless of size, proximity to the camera, orientation, occlusions, lighting, and noise.</i>
            <br>
          </center>
          <h3>Rapid Artificial Training Data Generation</h3>
          <p class="fontsize">
            To detect and manipulate objects with our robot at RIVeR, we used the Mask R-CNN framework for object detection and segmentation. In order to gather a sufficiently large amount of training data in an efficient and rapid manner, our approach involved generating an exhaustive artificial training set afflicted with various types of noise, inspired by <b>OpenAI's paper on domain randomization</b> (<a href="https://arxiv.org/abs/1703.06907">https://arxiv.org/abs/1703.06907</a>). By adding noise that may appear in the images taken by the camera, we train our object detection model to be more robust against them during inference.
            <br><br>
            To generate the artificial training images, <b>videos of each object rotating atop an automatic turntable are recorded.</b> This is done at various camera elevations to capture all angles of the object from different perspectives. Background subtraction is then performed on each frame of the video in order to extract all contours of the object. These extracted contours are then randomly chosen, scaled, and rotated, before being placed on an image of scenery similar to the types of environments the robot would operate in. For each image, <b>randomly chosen contours from a few randomly chosen classes are used to teach our model how to best distinguish the objects from each other when they appear together</b>, which is especially helpful for objects that are similar in appearance. <b>Occlusions, up to a certain percentage threshold, are allowed</b> and are properly recorded when annotations are created. Each of these composite images are then afflicted with random adjustments in lighting and artificial image noise. Once a composite is generated, a corresponding annotation that records the object instanceâ€™s class and the pixel-wise contour is created in COCO format. As a result, after training, <b>our model was robust in detecting objects regardless of size, proximity to the camera, orientation, occlusions, lighting, and noise.</b>
            <br><br>
            Using the segmentation masks, we were able to precisely segment each objects point clouds in 3D space, and successfully compute grasps in cluttered environments. This approach was implemented in the World Robot Summit in Tokyo. 
          </p>
          <h3>Results</h3>
          <center>
            <br>
            <img src="../img/maskrcnn/all_wrs.jpg" width="80%">
            <br>
            <i class="fontsize">Examples of artificial training data, exhibiting changes in noise </i>
            <br><br>
            <img src="../img/maskrcnn/lego_maskrcnn.jpg" width="80%">
            <br>
            <i class="fontsize">Lego blocks and a banana </i>
            <br><br>
            <img src="../img/maskrcnn/lab_maskrcnn.png" width="80%">
            <br>
            <i class="fontsize">Groceries to sort</i>
            <br><br>
            <video playsinline autoplay muted loop style="width: 80%" class="webby">
              <source src="https://github.com/naokiyokoyama/website_media/raw/master/mask_rcnn_robust_detections.mp4" type="video/mp4"></source>
            </video>
            <br>
            <i class="fontsize">Robustness to changes in orientation and occlusions. Inferences were generated on each frame after the video was recorded, they were not done in real-time. </i>
            <br><br>
            <div class="video-responsive">
              <iframe width="560" height="315" src="https://www.youtube.com/embed/LOxhekmqlNw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div> 
            <p class="fontsize"><i>Demonstration of real-time detections using the Toyota HSR.</i></p>
          </center>
          <div class="col-md-2"></div>
      </div>
    </div>  
    <script src="../js/bootstrap.js"></script>
  </body>
</html>
